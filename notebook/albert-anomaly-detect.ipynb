{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06843d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import glob\n",
    "import json \n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8576b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "training_dir = pathlib.Path(\"../training_data/\")\n",
    "positive_example = \"ls\"\n",
    "anomalies = set(next(os.walk(training_dir))[1]) - {positive_example}\n",
    "subdir = positive_example\n",
    "\n",
    "def load_dir(subdir):\n",
    "    sentences = []\n",
    "    print(str(training_dir / subdir / \"*json\"))\n",
    "    for fname in glob.glob(str(training_dir / subdir / \"*json\")):\n",
    "        with open(fname, \"r\") as fp:\n",
    "            sentence = json.load(fp)\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "positives = load_dir(positive_example)\n",
    "syscalls = positives\n",
    "for sd in anomalies:\n",
    "    syscalls = syscalls + load_dir(sd)\n",
    "labels = [1,] * len(positives) + [0,] * (len(syscalls) - len(positives))\n",
    "num_syscalls = max([max(s) for s in syscalls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train albert on the anomaly detection task\n",
    "class SysCallDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "model_name = \"albert-base-v2\"\n",
    "output_dir = \"../models/albert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Suppose your sequences of system calls are in a list of lists where each inner list is a sequence\n",
    "# And `labels` is a list of same length indicating whether each sequence is normal (0) or anomalous (1)\n",
    "sequences = syscalls\n",
    "labels = labels\n",
    "\n",
    "# Convert sequences to string as transformers Tokenizer requires string inputs\n",
    "sequences_str = [' '.join(map(str, seq)) for seq in sequences]\n",
    "\n",
    "# Tokenize sequences\n",
    "inputs = tokenizer(sequences_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Prepare labels\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "labels_tensor = torch.tensor(labels_encoded)\n",
    "\n",
    "# Convert to Dataset\n",
    "dataset = SysCallDataset(inputs, labels_tensor)\n",
    "\n",
    "# Setup training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    compute_metrics=None,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to onnx runtime \n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'logits': {0: 'batch_size'}\n",
    "}\n",
    "\n",
    "# After training, export model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (inputs['input_ids'], inputs['attention_mask']),\n",
    "    os.path.join(output_dir, \"model.onnx\"),\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=13,\n",
    ")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26eecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
